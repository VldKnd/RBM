# Main
### Elementary functions
An RBM can be represented by an object/structure containing a field W (weight matrix linking visible variables to hidden variables), a field a (bias of input units) and a field b (bias of output units)
A neural network (DNN) and a Deep Belief Network (DBN) can be represented by a list of RBMs, the size of this list being equal to the number of hidden layers of the network (+ classifying layer in the case of the DNN). Each element of this list will therefore coincide with an RBM and will contain a field W (weight matrix linking 2 consecutive layers), a field a (bias of the input units which coincide with the estimated variational parameters, except for the first layer) and a field b.
Here is a list of functions that will allow to build/learn neural network.
### Building an RBM and testing on Binary AlphaDigits
A main_RBM_alpha script allows you to learn the characters of the Binary AlphaDigits database of your choice via an RBM and to generate characters similar to those learned. The construction of this program requires the following functions:
* read_alpha_digit function that retrieves data in matrix form (row data, column pixels) and takes as argument the characters (or their index 0, - - - , 35) to "learn".
* init_RBM allowing to build and initialize the weights and biases of an RBM. This function will return an RBM structure with initialized weights and biases. The biases will be initialized to 0 while the weights will be initialized randomly according to a centered normal distribution, with a variance equal to 0.01.
* input_output_RBM function that takes an RBM structure and input data as arguments and returns the value of the output units calculated from the sigmoid function.
* output_input_RBM function that takes as arguments an RBM, output data and returns the value of the input units from the sigmoid function.
* train_RBM function that learns an RBM from the Contrastive-Divergence-1 algorithm in a non supervised manner. This function will return an RBM structure and will take as arguments an RBM structure, the number of iterations of the gradient descent (epochs), the learning rate, the size
of the mini-batch, input data... At the end of each gradient iteration, the squared error between the input data and the data reconstructed from the hidden unit will be displayed to measure the reconstruction power of the RBM.
* generer_image_RBM generates samples following an RBM. This function will return and display the generated images and will take as arguments an RBM structure, the number of iterations to use in the Gibbs sampler and the number of images to generate.
### Building a DBN and testing on Binary AlphaDigits
* main_DBN_alpha script to learn characters from the Binary AlphaDigits database. 
* init_DNN function to construct and initialize (possibly randomly) the weights and biases of a DNN. This function will return a DNN structure, will take the size of the network as an argument and can iteratively use the previous function.
* pretrain_DNN function to learn a DBN (Greedy layer wise procedure) in an unsupervised way. This function will return a pre-trained DNN and will take as arguments a DNN, the number of iterations of the gradient descent, the learning rate, the size of the mini-batch and the input data. We recall that the pre-training of a DNN can be seen as the successive training of RBMs. This function will therefore use train_RBM as well as entree_sortie_RBM.
* generer_image_DBN to generate samples following a DBN. This function will return and display the generated images and will take as arguments a pre-trained DNN, the number of iterations to use in the Gibbs sampler and the number of images to generate.